import os
import time
import threading
import requests
import psutil
import csv
from datetime import datetime
import warnings

# (선택) pynvml FutureWarning 숨기고 싶으면 주석 해제
# warnings.filterwarnings("ignore", category=FutureWarning)


# 0) 기본 설정
OLLAMA_BASE = "http://127.0.0.1:11434"
GENERATE_URL = f"{OLLAMA_BASE}/api/generate"
TAGS_URL = f"{OLLAMA_BASE}/api/tags"

MODELS = [
    "gemma3:1b",
    "gemma3:4b",
    "gemma3:12b",
]

INTERVAL_SEC = 0.4     # peak 샘플링 간격(0.1~0.5 권장)
TIMEOUT_SEC = 900      # 긴 요약 프롬프트 대비 넉넉하게

# 답변 저장 폴더
ANSWER_DIR = "answers"
os.makedirs(ANSWER_DIR, exist_ok=True)

#프롬프트
PROMPTS = {
    "writing": """‘기억의 왜곡’을 주제로 3문단 짧은 에세이를 작성하라. 첫 문단은 개념 정의, 두 번째는 실생활 사례, 세 번째는 한계점을 논하라.""",

    "coding": """Python으로 1부터 1000까지의 숫자 중 3의 배수이면서 5의 배수가 아닌 모든 숫자의 합을 구하는 코드를 작성하고, 결과를 설명해 줘.""",

    "summary": """문장이나 영상, 음성을 만들어 내는 인공 지능 생성 모델 중 확산 모델은 영상의 복원, 생성 및 변환에 뛰어난 성능을 보인다. 확산 모델의 기본 발상은, 원본 이미지에 노이즈를 점진적으로 추가하였다가 그 노이즈를 다시 제거해 나가면 원본 이미지를 복원할 수 있다는 것이다. 노이즈는 불필요하거나 원하지 않는 값을 의미한다. 원하는 값만 들어 있는 원본 이미지에 노이즈를 단계별로 더하면 노이즈가 포함된 확산 이미지가 되고, 여러 단계를 거치면 결국 원본 이미지가 어떤 이미지였는지 전혀 알아볼 수 없는 노이즈 이미지가 된다. 역으로, 단계별로 더해진 노이즈를 알 수 있다면 노이즈 이미지에서 원본 이미지를 복원할 수 있다. 확산 모델은 노이즈 생성기, 이미지 연산기, 노이즈 예측기로 구성되며, 순확산 과정과 역확산 과정 순으로 작동한다. 순확산 과정은 이미지에 노이즈를 추가하면서 노이즈 예측기를 학습시키는 과정이다. 첫 단계에서는, 노이즈 생성기에서 노이즈를 만든 후 이미지 연산기가 이 노이즈를 원본 이미지에 더해서 노이즈가 포함된 확산 이미지를 출력한다. 다음 단계부터는 노이즈 생성기에서 만든 노이즈를 이전 단계에서 출력된 확산 이미지에 더한다. 이러한 단계를 충분히 반복하면 최종적으로 노이즈 이미지가 출력된다. 이때 더해지는 노이즈는 크기나 분포 양상 등 그 특성이 단계별로 다르다. 따라서 노이즈 예측기는 단계별로 확산 이미지를 입력받아 이미지에 포함된 노이즈의 특성을 추출하여 수치들로 표현하고, 이 수치들을 바탕으로 노이즈를 예측한다. 노이즈 예측기 내부의 이러한 수치들을 잠재 표현이라고 한다. 노이즈 예측기는 잠재 표현을 구하고 노이즈를 예측하는 방식을 학습한다. 노이즈 예측기의 학습 방법은 기계 학습 중에서 지도 학습에 해당한다. 지도 학습은 학습 데이터에 정답이 주어져 출력과 정답의 차이가 작아지도록 모델을 학습시키는 방법이다. 노이즈 예측기를 학습시킬 때는 노이즈 생성기에서 만들어 넣어 준 노이즈가 정답에 해당하며 이 노이즈와 예측된 노이즈 사이의 차이가 작아지도록 학습시킨다. 역확산 과정은 노이즈 이미지에서 노이즈를 제거하여 원본 이미지를 복원하는 과정이다. 노이즈를 제거하려면 이미지에 단계별로 어떤 특성의 노이즈가 더해졌는지 알아야 하는데 노이즈 예측기가 이 역할을 한다. 노이즈 이미지 또는 중간 단계에서의 확산 이미지를 노이즈 예측기에 입력하면 이미지에 포함된 노이즈의 특성을 추출하여 잠재 표현을 구하고 이를 바탕으로 노이즈를 예측한다. 이미지 연산기는 입력된 확산 이미지로부터 이 노이즈를 빼서 현 단계의 노이즈를 제거한 확산 이미지를 출력한다. 확산 이미지에 이런 단계를 반복하면 결국 노이즈가 대부분 제거되어 원본 이미지에 가까운 이미지만 남게 된다. 한편, 많은 종류의 이미지를 학습시킨 후 학습된 이미지의 잠재 표현에 고유 번호를 붙이면 역확산 과정에서 이미지를 선택하여 생성할 수 있다. 또한 잠재 표현의 수치들을 조정하면 다른 특성의 노이즈가 생성되어 여러 이미지를 혼합하거나 실재하지 않는 이미지를 만들어 낼 수도 있다.
이 글을 요약해줘""",

    "conflict": """학교 프로젝트에서 팀원 간 역할 충돌이 생겼을 때 문제를 해결하는 방법을 ‘원인 분석 → 선택지 제시 → 해결 절차’ 순서로 설명해줘.""",

    "logic": """다음 논리 문제에서 옳은 결론을 도출하라: A가 참이면 B는 거짓이다. B가 참이면 C는 참이다. C가 거짓이면 A는 참이다. 이 세 명제의 참·거짓 관계를 모순 없이 정리하고 이유를 단계별로 설명하라."""
}

GPU_ENABLED = False
GPU_HANDLE = None

try:
    import pynvml
    try:
        pynvml.nvmlInit()
        GPU_ENABLED = True
        GPU_HANDLE = pynvml.nvmlDeviceGetHandleByIndex(0)
    except Exception:
        GPU_ENABLED = False
        GPU_HANDLE = None
except Exception:
    GPU_ENABLED = False
    GPU_HANDLE = None


def get_gpu_metrics():
    """(gpu_util_percent, vram_used_mb) / NVIDIA GPU 없으면 (None, None)"""
    if not GPU_ENABLED:
        return None, None
    util = pynvml.nvmlDeviceGetUtilizationRates(GPU_HANDLE).gpu
    mem_used_mb = pynvml.nvmlDeviceGetMemoryInfo(GPU_HANDLE).used / (1024 ** 2)
    return util, mem_used_mb

# 2) Ollama 서버/모델 체크
def check_ollama_server() -> bool:
    try:
        _ = requests.get(OLLAMA_BASE, timeout=2)
        return True
    except Exception:
        return False


def get_available_models() -> set:
    """Return set of model names via /api/tags (가능하면 모델 존재 여부 체크용)"""
    try:
        r = requests.get(TAGS_URL, timeout=5)
        r.raise_for_status()
        data = r.json()
        models = set()
        for m in data.get("models", []):
            name = m.get("name")
            if name:
                models.add(name)
        return models
    except Exception:
        return set()


# 3) 측정 모니터링
def monitor_peak(stop_event: threading.Event, interval_sec: float):
    peak_cpu = 0.0
    peak_ram = 0.0
    peak_gpu = 0.0 if GPU_ENABLED else None
    peak_vram = 0.0 if GPU_ENABLED else None

    sum_cpu = 0.0
    sum_ram = 0.0
    sum_gpu = 0.0 if GPU_ENABLED else None
    sum_vram = 0.0 if GPU_ENABLED else None
    n = 0

    psutil.cpu_percent(interval=None)

    while not stop_event.is_set():
        cpu = psutil.cpu_percent(interval=None)
        ram = psutil.virtual_memory().percent

        peak_cpu = max(peak_cpu, cpu)
        peak_ram = max(peak_ram, ram)

        sum_cpu += cpu
        sum_ram += ram
        n += 1

        if GPU_ENABLED:
            gpu, vram = get_gpu_metrics()

            if gpu is not None:
                peak_gpu = max(peak_gpu, gpu)
                sum_gpu += gpu

            if vram is not None:
                peak_vram = max(peak_vram, vram)
                sum_vram += vram

        time.sleep(interval_sec)

    avg_cpu = (sum_cpu / n) if n else 0.0
    avg_ram = (sum_ram / n) if n else 0.0
    avg_gpu = (sum_gpu / n) if (GPU_ENABLED and n) else None
    avg_vram = (sum_vram / n) if (GPU_ENABLED and n) else None

    return peak_cpu, peak_ram, peak_gpu, peak_vram, avg_cpu, avg_ram, avg_gpu, avg_vram


# 4) Ollama 실행 + 값 측정
def run_ollama_with_peak(model: str, prompt: str):
    payload = {"model": model, "prompt": prompt, "stream": False}

    stop_event = threading.Event()
    peak_result = {}

    def _mon():
        pc, pr, pg, pv, ac, ar, ag, av = monitor_peak(stop_event, INTERVAL_SEC)
        peak_result["peak_cpu"] = pc
        peak_result["peak_ram"] = pr
        peak_result["peak_gpu"] = pg
        peak_result["peak_vram"] = pv
        peak_result["avg_cpu"]  = ac
        peak_result["avg_ram"]  = ar
        peak_result["avg_gpu"]  = ag
        peak_result["avg_vram"] = av


    t = threading.Thread(target=_mon, daemon=True)
    t.start()

    t0 = time.time()
    res = requests.post(GENERATE_URL, json=payload, timeout=TIMEOUT_SEC)
    t1 = time.time()

    stop_event.set()
    t.join()

    res.raise_for_status()
    data = res.json()
    answer = data.get("response", "")
    elapsed = t1 - t0

    return (answer, elapsed,
        peak_result["peak_cpu"], peak_result["peak_ram"], peak_result["peak_gpu"], peak_result["peak_vram"],
        peak_result["avg_cpu"],  peak_result["avg_ram"],  peak_result["avg_gpu"],  peak_result["avg_vram"], )



def safe_filename(s: str) -> str:
    return s.replace(":", "_").replace("/", "_").replace("\\", "_").replace(" ", "_")


# 5) 실행
def main():
    # 서버 체크
    if not check_ollama_server():
        print(" Ollama 서버에 연결할 수 없습니다.")
        print(" 먼저 터미널에서:  ollama serve")
        return

    # 모델 체크(가능하면)
    available = get_available_models()
    if available:
        missing = [m for m in MODELS if m not in available]
        if missing:
            print(" 아래 모델이 Ollama에 없습니다(모델 이름/태그 확인 필요):")
            for m in missing:
                print("  -", m)
            print("\n 터미널에서 `ollama list`로 실제 모델 이름을 확인하고 MODELS를 맞춰주세요.")
            return
    else:
        print(" /api/tags에서 모델 목록을 못 가져왔습니다. 그래도 진행은 시도합니다(모델명이 정확해야 함).")

    # 결과 파일명
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = f"llm_peak_results_{ts}.csv"


    with open(out_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)

        writer.writerow([
            "model","prompt_type",
            "elapsed_sec",
            "peak_cpu%","peak_ram%","peak_gpu%","peak_vram_mb",
            "avg_cpu%","avg_ram%","avg_gpu%","avg_vram_mb",
            "answer_chars","answer_file"
        ])

        for model in MODELS:
            for ptype, prompt in PROMPTS.items():
                try:
                    (answer, elapsed,
                    peak_cpu, peak_ram, peak_gpu, peak_vram,
                    avg_cpu, avg_ram, avg_gpu, avg_vram) = run_ollama_with_peak(model, prompt)

                    # 답변 저장(txt) 
                    answer_filename = f"{safe_filename(model)}_{ptype}_run.txt"
                    answer_path = os.path.join(ANSWER_DIR, answer_filename)
                    with open(answer_path, "w", encoding="utf-8") as af:
                        af.write(answer)

                    # CSV 저장(peak + avg)
                    writer.writerow([
                        model, ptype,
                        round(elapsed, 3),

                        round(peak_cpu, 1),
                        round(peak_ram, 1),
                        (round(peak_gpu, 1) if peak_gpu is not None else ""),
                        (round(peak_vram, 1) if peak_vram is not None else ""),

                        round(avg_cpu, 1),
                        round(avg_ram, 1),
                        (round(avg_gpu, 1) if avg_gpu is not None else ""),
                        (round(avg_vram, 1) if avg_vram is not None else ""),

                        len(answer),
                        answer_path
                    ])

                    print(f" {model} | {ptype} | {elapsed:.2f}s | saved: {answer_path}")

                except KeyboardInterrupt:
                    print("\n 사용자가 중단했습니다. 현재까지 저장된 CSV/답변은 유지됩니다.")
                    return
                except requests.exceptions.ConnectionError:
                    print(" 연결 오류: Ollama 서버가 꺼졌거나 주소/포트가 다릅니다.")
                    print("   - ollama serve 실행 상태 유지")
                    print(f"   - 현재 URL: {GENERATE_URL}")
                    return
                except requests.HTTPError as e:
                    print(f" HTTP 오류: {e}")
                    print("   모델명이 실제와 다를 가능성이 큽니다. `ollama list`로 확인하세요.")
                    return
                except Exception as e:
                    print(f" 기타 오류: {e}")
                    return

    print(f"\n 완료! 결과 CSV 저장됨: {out_csv}")
    print(f" 답변(txt) 저장 폴더: {ANSWER_DIR}/")
    if not GPU_ENABLED:
        print("NVIDIA GPU 측정이 비활성화되어 GPU/VRAM 열은 빈칸일 수 있습니다(정상).")


if __name__ == "__main__":
    main()
